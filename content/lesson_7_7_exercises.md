# Lesson 7.7 — Exercises (Hands‑On Lab for RAG Evaluation)

> **Chapter context:** You’ve completed 7.1–7.6. This lab turns the ideas into muscle memory. You will synthesize a dataset, compute retrieval metrics, evaluate generation quality with judges, calibrate to estimate **true success with CIs**, and wire everything into a simple CI gate. The tasks are realistic for a product help‑center RAG, multilingual (pt‑BR and es‑AR), and designed so you can complete them incrementally.

---

## How to use this lab

- Treat each section as a **milestone**. You can ship after Milestone 4; Milestones 5–9 push robustness.  
- You’ll work with **JSONL** files and minimal Python glue.  
- Each task includes: **Goal**, **Inputs**, **Steps**, **Deliverables**, **Acceptance Criteria (AC)**, and **Hints**.  
- Keep all artifacts under a single folder: `rag_eval_lab/`.

**Folder scaffold**

```
rag_eval_lab/
 ├─ data/
 │   ├─ corpus/                # raw docs or exported chunks
 │   ├─ dataset.jsonl          # queries + gold + metadata (7.2 schema)
 │   ├─ dataset_card.md
 │   └─ splits/                # train|val|test jsonl (time + group aware)
 ├─ runs/
 │   ├─ retrieval_run.jsonl    # top-k per query + latency
 │   ├─ answers.jsonl          # model answers with [doc#chunk] citations
 │   └─ report/                # HTML/MD report generated by scripts
 ├─ judges/
 │   ├─ generation_judge.txt   # prompt
 │   └─ relevance_judge.txt
 ├─ scripts/
 │   ├─ synth_dataset.py
 │   ├─ eval_retrieval.py
 │   ├─ eval_generation.py
 │   └─ make_report.py
 └─ ci_gates.yaml
```

---

## Milestone 0 — Warm‑up checks (10 min)

- Install dependencies (Python 3.10+).  
- Confirm you can read/write JSONL and run a small script.  
- Print “hello” artifact versions: dataset version, index version, judge version, code SHA (stub).

**AC:** A `runs/hello.md` with the versions you chose.

---

## Milestone 1 — Build a Mini Dataset (7.2 in practice)

**Goal:** Create **≥ 200 items** balanced across topics and languages with **multi‑hop** and **no‑answer** coverage.

**Inputs**
- 30–50 help‑center docs (or your own domain). Include **policy + exceptions**, **procedures**, **definitions**.  
- Language focus: pt‑BR and es‑AR (include at least 30% in each).

**Steps**
1. **Chunk** docs by headings + sentences (target 400–800 tokens; 60–120 token overlap). Save `chunk_id=doc#anchor`.  
2. **Generate questions** from chunks using the prompt in 7.2 (add noise and code‑switching).  
3. **Draft ideal answers** **only** from gold evidence (≤6 sentences; include exceptions).  
4. **Add hard negatives** (top BM25/dense distractors not in gold).  
5. **Create no‑answer items** (10–15%): questions whose answer is not present; add an **ideal abstain** template.  
6. **Validate** (schema, language match, evidence coverage, style length).  
7. **Judge‑accept** with a short LLM‑as‑judge (must‑pass + scores).  
8. **Split** time‑aware (`as_of`) and group‑aware (doc/section + template family): `train/val/test` (60/20/20).  
9. Write a **Dataset Card** (`data/dataset_card.md`).

**Deliverables**
- `data/dataset.jsonl` and `data/splits/*.jsonl`  
- `dataset_card.md` with: counts per **topic × intent × language × difficulty**, and known limitations.

**AC**
- ≥ 200 items; ≥ 50 multi‑hop; ≥ 30 no‑answer; ≥ 60 es‑AR; ≥ 60 pt‑BR.  
- Each item has ≥ 2 hard negatives (except no‑answer).  
- Automated checks: **100% pass**; judge acceptance rate **≥ 85%**.

**Hints**
- Start with documents‑only synthesis for coverage; then seed 30–50 items from real or realistic logs.  
- Keep **stable IDs**. Many later metrics depend on them.

---

## Milestone 2 — Retrieval Panel (7.3 in practice)

**Goal:** Produce decision‑grade retrieval numbers with CIs and **k‑sweep** curves.

**Inputs**
- `data/splits/test.jsonl` (or `val.jsonl` while iterating)  
- A retrieval system (BM25, dense, or hybrid) + optional reranker

**Steps**
1. Run retrieval (with reranker if you have it). Save JSONL per query:
   ```json
   {"query_id":"...", "topk":[{"rank":1,"chunk_id":"doc#p5","score":19.2,"doc_version":"2025-05-01"}, ...],
    "latency_ms":{"retrieve":35,"rerank":22}, "system":"hybrid_v1+crossenc_base","index_version":"v2025_06_01"}
   ```
2. Compute **Hit/Recall@{1,3,5,10,20}**, **MRR**, **nDCG@5**, **Precision@5**, **Distinct‑docs@5**.  
3. Plot **k‑sweep** for Recall/nDCG; compute **AUC** summary.  
4. Bootstrap **by query** to get 95% CIs.  
5. Slice by **language, topic, difficulty, answer_type**.  
6. Add **freshness** metric (fraction of retrieved items whose `doc.version ≤ as_of`).

**Deliverables**
- `runs/retrieval_run.jsonl` and `runs/report/retrieval_panel.md` (tables + curves).

**AC**
- Report includes CIs and segments.  
- At least one insight backed by the curve (e.g., “nDCG plateaus at k=5; we can keep k=5 and save cost”).

**Hints**
- If Recall@50 is low, inspect **doc‑level recall**. Often chunking is the issue, not embeddings.

---

## Milestone 3 — Reranker Ablation

**Goal:** Prove whether reranking helps and by how much.

**Steps**
1. Save **raw top‑50** from the retriever.  
2. Apply reranker → **top‑5**.  
3. Compute **Δ nDCG@5** and **Δ Precision@5** (with CIs) between raw and reranked.  
4. Show **examples** where reranker helped and where it hurt (2 each).

**AC**
- If reranker helps, **Δ nDCG@5 ≥ +0.10** with CI lower bound > 0 for at least one key segment; otherwise justify why to drop it.

---

## Milestone 4 — Generation Panel (7.4 in practice)

**Goal:** Evaluate answers for **groundedness, completeness, directness, style**, and **attribution** with **true success** correction.

**Inputs**
- `runs/retrieval_run.jsonl`  
- Generator that outputs **citations** (e.g., `[doc_123#p5]`)

**Steps**
1. Produce answers for all test queries. Save as:
   ```json
   {"query_id":"...","answer":"... [doc_123#p5][doc_123#p6]","tokens":{"prompt":900,"gen":120}}
   ```
2. Build a **digest** per query: top‑k evidence IDs with concise snippets.  
3. Run deterministic checks: **citation_correctness** and **supported_claims_rate**.  
4. Run the **generation judge** (must‑pass + 1–5 scores).  
5. Compute **end‑to‑end pass** (must‑pass true AND all scores ≥ 4).  
6. Calibrate: label **200** items by humans (or you + a colleague). Estimate judge `(sensitivity, specificity)` and compute **bias‑corrected true success** with CIs (bootstrap by **query**).  
7. Slice by segments and no‑answer items (false‑answer rate).

**Deliverables**
- `runs/answers.jsonl`  
- `runs/report/generation_panel.md` with tables (raw pass, true success + CIs), attribution metrics, and examples.

**AC (suggested gates)**
- **True success (CI lower) ≥ 0.80 overall; ≥ 0.75** for pt‑BR & es‑AR.  
- **Citation correctness ≥ 0.95; supported‑claims ≥ 0.90.**  
- **No‑answer false‑answer rate ≤ 2% (CI upper bound).**

**Hints**
- Keep the judge prompt **short** (≤12 lines), deterministic (temperature=0), and **digest‑based** (not full docs).

---

## Milestone 5 — No‑Answer & Abstention Hardening

**Goal:** Ensure the model refuses cleanly when the corpus lacks the answer.

**Steps**
1. Add **≥ 40** `no_answer=true` items.  
2. Tune the generator with an **evidence sufficiency** rule (“If key facts are missing, abstain”).  
3. Measure **false‑answer rate** and **abstain_quality** (1–5).  
4. Show **before/after** with CIs.

**AC**
- CI upper bound of false‑answer rate ≤ 0.02; median abstain_quality ≥ 4.0.

---

## Milestone 6 — Segment Robustness

**Goal:** Find weak segments and propose fixes.

**Steps**
1. Segment by **language (pt‑BR, es‑AR)** and **topic** (your top 3).  
2. Report Retrieval Recall@5, nDCG@5, Generation True Success (CIs).  
3. Identify the **worst segment** and run a **targeted fix** (e.g., locale synonyms in retrieval, prompt with tone per locale, chunk “rule + exceptions”).  
4. Re‑measure; explain **why** the fix worked using diagnostics.

**AC**
- ≥ +0.05 absolute improvement in the weak segment on at least one key metric with CI lower bound > 0.

---

## Milestone 7 — Drift & Freshness

**Goal:** Prove the system respects `as_of` and survives policy updates.

**Steps**
1. Pick **10** docs; change policy details, bump their `version/as_of`.  
2. Regenerate affected dataset items (7.2 workflow).  
3. Re‑run retrieval & generation panels **with freshness filters**.  
4. Report how metrics change and where failures concentrate.

**AC**
- **Freshness** metric ≥ 0.99 (almost no look‑ahead leakage).  
- A short write‑up on how you’ll monitor drift in production.

---

## Milestone 8 — CI Gates & Release Simulation

**Goal:** Automate guards that block risky releases.

**Steps**
1. Create `ci_gates.yaml` like:
   ```yaml
   retrieval:
     recall_at_5: { overall: 0.90, ptBR: 0.85, esAR: 0.85 }
     ndcg_at_5:   { overall: 0.80 }
     precision_at_5: { overall: 0.60 }
   generation:
     true_success: { overall: 0.80, ptBR: 0.75, esAR: 0.75 }
     citation_correctness: { overall: 0.95 }
     no_answer_false_rate_max: 0.02
   latency:
     retrieve_p50_ms: 60
     rerank_p50_ms: 50
   ```
2. In `make_report.py`, read the gates and **fail the build** if any **CI lower** bound is below threshold.  
3. Simulate a release: change something (retriever weights, chunk size, prompt). Run the full pipeline; confirm the gates catch regressions.  
4. Produce a **before/after** table with **Δ** and call the decision (ship or not).

**AC**
- Build fails on intentional regressions; passes when fixed.  
- A single consolidated **MD/HTML report** is generated in `runs/report/` with **artifact versions** (dataset/index/judges/code SHA).

---

## Milestone 9 — Pitfall Bingo (7.5 in practice)

**Goal:** Proactively search for landmines and file issues to fix them.

**Steps**
- Run the **8 quick tests** from 7.5: k‑sweep, doc‑vs‑chunk recall, hard‑negative drop, minimal‑pair verbosity, no‑answer pack, freshness filter, judge stability (5×), segment heatmap.  
- For each failing test, file a 1‑page **investigation note**: *symptom → hypothesis → evidence → fix plan*.  
- Prioritize the top 3 by impact/risk.

**AC**
- At least two pitfalls identified with concrete fix plans and owners.

---

## Appendix A — Schemas (copy/paste)

### A.1 Dataset item (JSONL)
```json
{
  "query_id": "q_000042",
  "question": "¿Puedo pedir reembolso después de 30 días?",
  "language": "es-AR",
  "intent": "policy_lookup",
  "answer_type": "short_fact",
  "difficulty": "medium",
  "as_of": "2025-05-01",
  "gold_evidence": ["doc_123#p5", "doc_123#p6"],
  "ideal_answer": "No, salvo defecto de fabricación dentro de 90 días.",
  "citations": ["doc_123#p5", "doc_123#p6"],
  "negatives": ["doc_123#p8", "doc_777#p2"],
  "multi_hop": {"requires": 2, "hops": [{"evidence":"doc_123#p5"}, {"evidence":"doc_123#p6"}]},
  "no_answer": false,
  "tags": ["refunds","AR"]
}
```

### A.2 Retrieval run (per query)
```json
{"query_id":"q_000042","topk":[{"rank":1,"chunk_id":"doc_123#p6","score":18.4,"doc_version":"2025-04-01"},{"rank":2,"chunk_id":"doc_123#p5","score":17.9,"doc_version":"2025-04-01"}],"latency_ms":{"retrieve":35,"rerank":22},"index_version":"v2025_06_01","system":"hybrid_v3+crossenc_base"}
```

### A.3 Answer record
```json
{"query_id":"q_000042","answer":"No después de 30 días, salvo defecto de fabricación hasta 90 días. [doc_123#p5][doc_123#p6]","tokens":{"prompt":860,"gen":95},"model":"generator_x_y"}
```

---

## Appendix B — Judges (prompts)

### B.1 Generation judge (JSON‑only)

```
You evaluate an answer for a Retrieval-Augmented QA system. Use ONLY the EVIDENCE below.
Return strict JSON with:
- must_pass{schema_valid,language_match,safety_pass,citations_present,citations_valid,no_hallucinated_entities}
- scores{groundedness,completeness,directness,style} (1-5)
- supported_claims[] with evidence IDs
- abstain{should_have_abstained,abstain_quality}
```

### B.2 Relevance judge (2/1/0)

```
Is the chunk relevant to answering the question? 
Return JSON: {"relevance": 2|1|0, "rationale": "..."}
```

---

## Appendix C — Bootstrap helper (pseudo‑Python)

```python
import random
def bootstrap_mean(xs, n=2000, unit="query"):
    m = len(xs)
    boots = []
    for _ in range(n):
        sample = [xs[random.randrange(m)] for _ in range(m)]
        boots.append(sum(sample)/m)
    boots.sort()
    return {
        "mean": sum(xs)/m,
        "ci_lower": boots[int(0.025*n)],
        "ci_upper": boots[int(0.975*n)]
    }
```

---

## What “great” looks like (rubric)

- **Dataset quality (30%)**: coverage, balance, acceptance rate, dataset card clarity.  
- **Retrieval diagnostics (20%)**: clean curves, segment insights, justified `k`.  
- **Generation evaluation (30%)**: judge implemented, calibration done, true success reported with CIs, attribution metrics strong.  
- **CI gates (10%)**: correct thresholds, build fails on regressions, artifact versions captured.  
- **Communication (10%)**: crisp report with examples that teach others what changed and why.

---

## Final note

This lab is designed so each milestone adds one reliable brick to your evaluation system. If you follow the acceptance criteria and keep CI **gating on CI‑lower bounds**, you’ll have an evaluation pipeline that a real product team can trust day‑to‑day.

**When you’re ready, run Milestone 8 (CI) end‑to‑end and attach the report in your next message for review.**
